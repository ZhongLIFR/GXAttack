#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr 28 10:30:44 2024

@author: Anonymous

This testbed is used to test the transferrability of attacks generated by GXAttack (from PGExplainer to  other explainers)

            
"""
import argparse

parser = argparse.ArgumentParser(description='Save a dataset with a specified filename from command line.')
parser.add_argument('--dataset_name', type=str, help='Name of the dataset file to save.')
args = parser.parse_args()

# =============================================================================
# Add this code block if there is a fault related to "_centered"
# =============================================================================
import numpy as np
import  scipy.signal.signaltools

def _centered(arr, newsize):
    # Return the center newsize portion of the array.
    newsize = np.asarray(newsize)
    currsize = np.array(arr.shape)
    startind = (currsize - newsize) // 2
    endind = startind + newsize
    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]
    return arr[tuple(myslice)]

scipy.signal.signaltools._centered = _centered



import sys
import graphxai


import torch
import matplotlib.pyplot as plt
from graphxai.datasets import ShapeGGen


# =============================================================================
# Step1. Load the dataset and run a quick plot of the dataset.
# =============================================================================


# --------------------------------------------
# Load the dataset from the file
# --------------------------------------------
save_path = f'./exp_results/{args.dataset_name}.pth'
dataset = torch.load(save_path)

plt.figure(figsize = (8, 8))
dataset.visualize(show = False)


# --------------------------------------------
# set random seed for reproducible results: not working for generating dataset
# --------------------------------------------
import torch
import random
import os
import numpy as np

seed_number = 2

## Set random seed
np.random.seed(seed_number)
torch.manual_seed(seed_number)
torch.cuda.manual_seed(seed_number)
torch.cuda.manual_seed_all(seed_number)
random.seed(seed_number)
os.environ['PYTHONHASHSEED'] = str(seed_number)
os.environ['OMP_NUM_THREADS'] = '1'
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False


# =============================================================================
# Step2:  Build a GNN predictor using GCNConv, which can accept edge weight
# =============================================================================    
from torch_geometric.nn import GCNConv

##MyGNN class so that it can handle situations: 1) edge_weight are provided, 2) edge_weight are not provided
class MyGNN(torch.nn.Module):
    def __init__(self, input_feat, hidden_channels, classes=2):
        super(MyGNN, self).__init__()
        # Initialize the first GCN layer
        self.gcn1 = GCNConv(in_channels=input_feat, out_channels=hidden_channels)
        # Initialize the second GCN layer
        self.gcn2 = GCNConv(in_channels=hidden_channels, out_channels=classes)

    def forward(self, x, edge_index, edge_weight=None):
        # Pass the input through the first GCN layer and apply a ReLU activation
        if edge_weight is not None:
            x = self.gcn1(x, edge_index, edge_weight)
        else:
            x = self.gcn1(x, edge_index)
        x = torch.relu(x)

        # Pass the result through the second GCN layer
        if edge_weight is not None:
            x = self.gcn2(x, edge_index, edge_weight)
        else:
            x = self.gcn2(x, edge_index)
        return x


# =============================================================================
# Step3: Split the data and train the GNN predictor
# =============================================================================

from torch_geometric.data import Data
import sklearn.metrics as metrics
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

##define a function to train the GNN predictor
def predictor_train(model: torch.nn.Module, 
                    optimizer,
                    criterion, 
                    data):
    model.train()
    
    # Clear gradients.
    optimizer.zero_grad() 
    
    # Perform a single forward pass.
    out = model(data.x, data.edge_index, data.edge_attr)  
    
    # Compute the loss solely based on the training nodes.
    loss = criterion(out[data.train_mask], data.y[data.train_mask]) 
    
    # Derive gradients.
    loss.backward()
    
    # Update parameters based on gradients.
    optimizer.step()  
    
    return loss
 
def predictor_test(model: torch.nn.Module, data, num_classes=2, get_auc=False):
    model.eval()
    
    # Perform a single forward pass.
    out = model(data.x, data.edge_index, data.edge_attr)
    
    # Use the class with the highest probability.
    pred = out.argmax(dim=1)
    
    # Get the predicted classes by finding the indices of the max logit for each example
    _, predicted_classes = out.max(dim=1)
    
    # Gather the probabilities of the predicted classes
    probabilities = out.softmax(dim=1)
    
    # Initialize sets for correctly and wrongly predicted nodes and for the specific probability ranges.
    correct_nodes, wrong_nodes = set(), set()
    proba_ranges = {f'range_{i}': set() for i in range(5, 10)}

    # Extract test indices to match predictions with original indices
    test_indices = data.test_mask.nonzero(as_tuple=False).squeeze()

    # Select the probabilities of the predicted classes for the test dataset
    probas_pred = probabilities[test_indices, predicted_classes[test_indices]].detach().numpy()
    
    # Get true labels for the test dataset using the test indices
    true_Y = data.y[test_indices].numpy()

    # Calculate accuracy.
    acc = accuracy_score(true_Y, pred[test_indices].numpy())

    # Fill in the sets based on the predictions and probability ranges for original indices
    for idx, (true_label, predicted_label, proba) in zip(test_indices, zip(true_Y, pred[test_indices].numpy(), probas_pred)):
        original_idx = idx.item()  # Get the original node index
        if true_label == predicted_label:
            correct_nodes.add(original_idx)
        else:
            wrong_nodes.add(original_idx)
        
        # Assign nodes to the specific probability range sets based on probability
        if 0.5 <= proba < 0.6:
            proba_ranges['range_5'].add(original_idx)
        elif 0.6 <= proba < 0.7:
            proba_ranges['range_6'].add(original_idx)
        elif 0.7 <= proba < 0.8:
            proba_ranges['range_7'].add(original_idx)
        elif 0.8 <= proba < 0.9:
            proba_ranges['range_8'].add(original_idx)
        elif 0.9 <= proba <= 1.0:
            proba_ranges['range_9'].add(original_idx)

    # Calculate metrics for binary classification if applicable.
    if num_classes == 2:
        test_score = f1_score(true_Y, pred[test_indices].numpy())
        precision = precision_score(true_Y, pred[test_indices].numpy())
        recall = recall_score(true_Y, pred[test_indices].numpy())

        # Calculate AUROC and AUPRC if required.
        if get_auc:
            auprc = metrics.average_precision_score(true_Y, probas_pred, pos_label=1)
            auroc = metrics.roc_auc_score(true_Y, probas_pred)

            return acc, test_score, precision, recall, auprc, auroc, correct_nodes, wrong_nodes, proba_ranges['range_5'], proba_ranges['range_6'], proba_ranges['range_7'], proba_ranges['range_8'], proba_ranges['range_9']
    
    return acc, test_score, correct_nodes, wrong_nodes, proba_ranges['range_5'], proba_ranges['range_6'], proba_ranges['range_7'], proba_ranges['range_8'], proba_ranges['range_9']


data = dataset.get_graph(use_fixed_split=True)

prediction_model = MyGNN(dataset.n_features, 32)
optimizer = torch.optim.Adam(prediction_model.parameters(), lr = 0.001, weight_decay = 0.001)
criterion = torch.nn.CrossEntropyLoss()

#--------------------------------------------
# Train the GNN classifer 
#--------------------------------------------

# Generate meaningful weights for original graph
edge_weights = torch.ones(data.edge_index.size(1), dtype=torch.float)

# Add edge weights to the Data object
data.edge_attr = edge_weights

for _ in range(300):
    loss = predictor_train(prediction_model, optimizer, criterion, data)

#--------------------------------------------
# Test the GNN classifer
#--------------------------------------------

##we use all nodes as test nodes here (normaly we should not do this)
data.test_mask =  torch.tensor([True] * data.x.shape[0])

##Zhong: modifications here, the order of acc and test_score
acc, f1, prec, rec, auprc, auroc, correct_nodes_set, wrong_nodes_set, prob5, prob6, prob7, prob8, prob9\
    = predictor_test(prediction_model, data, num_classes = 2, get_auc = True)

print('Test Accuracy: {:.4f}'.format(acc))
print('Test F1 score: {:.4f}'.format(f1))
print('Test AUROC: {:.4f}'.format(auroc))


# =============================================================================
# Step4: Train the GNN explainers: PGExplainer DIY by us
# =============================================================================


from torch_geometric.data import Data
from PGExplainer_WB import PGExplainer


# --------------------------------------------------------------
# Step4.1: Train PGExplainer
# --------------------------------------------------------------

# Embedding layer name is final GNN embedding layer in the model
explanation_model = PGExplainer(prediction_model, emb_layer_name = 'gcn2', max_epochs = 10, lr = 0.1)

# Required to first train PGExplainer on the dataset: feed the entire data
explanation_model.train_explanation_model(data)

# =============================================================================
# Step5: Attack the PGExplainer
# =============================================================================

from GXAttacker import PGExplainerAttack

# --------------------------------------------------------------
# Step5.1: Specify PGExplainerAttack
# --------------------------------------------------------------

# Embedding layer name is final GNN embedding layer in the prediction_model
attack_model = PGExplainerAttack(prediction_model, explanation_model, emb_layer_name = 'gcn2', max_epochs = 100, lr = 0.1)


# # =============================================================================
# # Step5.2:  Compute explanation accuracy change after perturbations for different explainers
# # =============================================================================

from graphxai.metrics import graph_exp_acc
                 
attack_results_prob5 = []

combined_probs = prob5.union(prob6, prob7, prob8, prob9)

# for node_idx in prob5:

for node_idx in combined_probs:
    
    ground_truth_exp_subgraph = dataset.explanations[node_idx]
    
    
    print("\n node_idx to attack: " + str(node_idx))
        
    # --------------------------------
    # before perturbation
    # --------------------------------
    
    # Get explanations from PGEx
    pgex_exp_subgraph_original, original_edge_mask = explanation_model.get_explanation_node(dataset = data,
                                                                                            node_idx = node_idx,
                                                                                            x = data.x,
                                                                                            edge_index = data.edge_index,
                                                                                            edge_weights = data.edge_attr)
    original_edge_mask_copy = original_edge_mask
    
    ##set the topk to 1 while the remaining to zeros
    topk_edges = torch.count_nonzero(ground_truth_exp_subgraph[0].edge_imp)
    topk_percent = int(0.25*data.edge_index.size(1))
    
    pgex_exp_subgraph_original.edge_imp = torch.where(torch.isin(torch.arange(len(pgex_exp_subgraph_original.edge_imp)), torch.topk(pgex_exp_subgraph_original.edge_imp, topk_edges).indices), torch.tensor(1.0), torch.tensor(0.0))
    original_edge_mask = torch.where(torch.isin(torch.arange(len(original_edge_mask)), torch.topk(original_edge_mask, topk_percent).indices), torch.tensor(1.0), torch.tensor(0.0))
    
    
    ## Accuracy of PGExplainer before perturbation
    pg_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], generated_exp = pgex_exp_subgraph_original)
    
    
    # --------------------------------
    # after perturbation of GXAttacker
    # --------------------------------
    
    # Run PGExplainerAttack to perform attack
    perturbed_result = attack_model.node_explanation_attack_model(dataset = data, node_idx_to_attack = node_idx)
        
    used_budget = perturbed_result[2]
    
    #Create a mask for non-zero weights
    non_zero_mask = perturbed_result[1] != 0
    
    # Apply the mask to edge_index to filter out edges with zero weights
    perturbed_edge_index = perturbed_result[0][:, non_zero_mask]
    
    # Apply the same mask to edge_weights to retain corresponding non-zero weights
    perturbed_edge_weights = perturbed_result[1][non_zero_mask]
    
    
    from copy import deepcopy
    perturbed_data = deepcopy(data)
    perturbed_data.edge_index = perturbed_edge_index
    perturbed_data.edge_attr =  perturbed_edge_weights
    
    # --------------------------------
    # # Explainer1: PGExplainer
    # --------------------------------
    pgex_exp_subgraph_perturbed, perturbed_edge_mask = explanation_model.get_explanation_node(dataset = perturbed_data,
                                                                                              node_idx = node_idx,
                                                                                              x = data.x,
                                                                                              edge_index = perturbed_edge_index,
                                                                                              edge_weights = perturbed_edge_weights)
       
    ## Accuracy of PGExplainer after perturbation
    pg_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], generated_exp = pgex_exp_subgraph_perturbed)
    
    
    print('Explanation Accuracy Change for PGExplainer:\n {:.4f} -> {:.4f}'.format(pg_acc_original,pg_acc_perturbed))
                  
    # --------------------------------
    # # Explainer2: GradExplainer
    # --------------------------------
    from graphxai.explainers import GradExplainer
        
    # No training with GradExplainer, just run the model:
    grad_explainer = GradExplainer(prediction_model, criterion = criterion)
    
    ##before perturbation
    grad_exp_original = grad_explainer.get_explanation_node(node_idx = node_idx, 
                                                            x = data.x, 
                                                            edge_index = data.edge_index, 
                                                            y = data.y)
    
    grad_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = grad_exp_original)

    ##after perturbation
    grad_exp_perturbed = grad_explainer.get_explanation_node(node_idx = node_idx, 
                                                             x = data.x, 
                                                             edge_index = perturbed_edge_index, 
                                                             y = data.y)
    grad_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                       generated_exp = grad_exp_perturbed)
    
    print('Explanation Accuracy Change for Gradients:\n {:.4f} -> {:.4f}'.format(grad_acc_original, grad_acc_perturbed)) 
    
    # --------------------------------
    # # Explainer3: GradCAM
    # --------------------------------
    from graphxai.explainers import GradCAM
        
    # No training with GradCAM, just run the model:
    gCAM_explainer = GradCAM(prediction_model)
    
    ##before perturbation
    gCAM_exp_original = gCAM_explainer.get_explanation_node(node_idx = node_idx, 
                                                            x = data.x, 
                                                            edge_index = data.edge_index, 
                                                            y = data.y)
    
    gCAM_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = gCAM_exp_original)

    ##after perturbation
    gCAM_exp_perturbed = gCAM_explainer.get_explanation_node(node_idx = node_idx, 
                                                           x = data.x, 
                                                           edge_index = perturbed_edge_index, 
                                                           y = data.y)
    gCAM_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = gCAM_exp_perturbed)
    
    print('Explanation Accuracy Change for GradCAM:\n {:.4f} -> {:.4f}'.format(gCAM_acc_original, gCAM_acc_perturbed))
    
    # --------------------------------
    # # Explainer4: GuidedBP
    # --------------------------------
    from graphxai.explainers import GuidedBP
        
    # No training with GuidedBP, just run the model:
    gbp_explainer = GuidedBP(prediction_model, criterion=criterion)
    
    ##before perturbation
    gbp_exp_original = gbp_explainer.get_explanation_node(node_idx = node_idx, 
                                                          x = data.x, 
                                                          edge_index = data.edge_index, 
                                                          y = data.y)
    
    gbp_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                     generated_exp = gbp_exp_original)

    ##after perturbation
    gbp_exp_perturbed = gbp_explainer.get_explanation_node(node_idx = node_idx, 
                                                           x = data.x, 
                                                           edge_index = perturbed_edge_index, 
                                                           y = data.y)
    gbp_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = gbp_exp_perturbed)
    
    print('Explanation Accuracy Change for GuidedBP:\n {:.4f} -> {:.4f}'.format(gbp_acc_original, gbp_acc_perturbed)) 
    
    # --------------------------------
    # # Explainer5: IntegratedGradExplainer
    # --------------------------------
    from graphxai.explainers import IntegratedGradExplainer
        
    # No training with Integrated IntegratedGradExplainer, just run the model:
    igex = IntegratedGradExplainer(prediction_model, criterion=criterion)
    
    ##before perturbation
    ig_exp_original = igex.get_explanation_node(node_idx = node_idx, 
                                                x = data.x, 
                                                edge_index = data.edge_index, 
                                                edge_weight = data.edge_attr,
                                                y = data.y)
    ig_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                    generated_exp = ig_exp_original, 
                                    threshold = ig_exp_original.node_imp.max().item() * 0.8)

    ##after perturbation
    ig_exp_perturbed = igex.get_explanation_node(node_idx = node_idx, 
                                                 x = data.x, 
                                                 edge_index = perturbed_edge_index, 
                                                 edge_weight = perturbed_edge_weights,
                                                 y = data.y)
    ig_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                     generated_exp = ig_exp_perturbed, 
                                     threshold = ig_exp_perturbed.node_imp.max().item() * 0.8)
    
    print('Explanation Accuracy Change for IGExplainer:\n {:.4f} -> {:.4f}'.format(ig_acc_original, ig_acc_perturbed))
    
    # --------------------------------
    # # Explainer6: GNNExplainer
    # --------------------------------
    from graphxai.explainers import GNNExplainer
        
    # No training with GNNExplainer, just run the model:
    gnn_explainer = GNNExplainer(prediction_model)
    
    ##before perturbation
    gnnE_exp_original = gnn_explainer.get_explanation_node(node_idx = node_idx, 
                                                         x = data.x, 
                                                         edge_index = data.edge_index, 
                                                         y = data.y)
    gnnE_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = gnnE_exp_original)

    ##after perturbation
    gnnE_exp_perturbed = gnn_explainer.get_explanation_node(node_idx = node_idx, 
                                                            x = data.x, 
                                                            edge_index = perturbed_edge_index, 
                                                            y = data.y)
    gnnE_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                       generated_exp = gnnE_exp_perturbed)
    
    print('Explanation Accuracy Change for GNNExplainer:\n {:.4f} -> {:.4f}'.format(gnnE_acc_original, gnnE_acc_perturbed)) 
    
 
    # --------------------------------
    # # Explainer7: SubgraphX (some issues in the implementation)
    # --------------------------------    
    from graphxai.explainers import SubgraphX

    # Initialize SubgraphX with your prediction model
    subX_explainer = SubgraphX(prediction_model)
    
    try:
        # Before perturbation
        subX_exp_original = subX_explainer.get_explanation_node(node_idx=node_idx, 
                                                                x=data.x, 
                                                                edge_index=data.edge_index, 
                                                                y=data.y)
        subX_acc_original = graph_exp_acc(gt_exp=ground_truth_exp_subgraph[0], 
                                          generated_exp=subX_exp_original)
        
        # After perturbation
        subX_exp_perturbed = subX_explainer.get_explanation_node(node_idx=node_idx, 
                                                                 x=data.x, 
                                                                 edge_index=perturbed_edge_index, 
                                                                 y=data.y)
        subX_acc_perturbed = graph_exp_acc(gt_exp=ground_truth_exp_subgraph[0], 
                                           generated_exp=subX_exp_perturbed)
        
    except IndexError as e:
        print(f"Error processing node index {node_idx} (original): {e}")
        subX_acc_original = 0
        subX_acc_perturbed = 0
    
    print('Explanation Accuracy Change for SubgraphX:\n {:.4f} -> {:.4f}'.format(subX_acc_original, subX_acc_perturbed))


    # --------------------------------
    # # Explainer8: PGMExplainer
    # --------------------------------
    from graphxai.explainers import PGMExplainer
        
    # No training with Integrated Gradients, just run the model:
    PGM_explainer = PGMExplainer(prediction_model, explain_graph=False)
    
    ##before perturbation
    PGM_exp_original = PGM_explainer.get_explanation_node(node_idx = node_idx, 
                                                             x = data.x, 
                                                             edge_index = data.edge_index)
    
    PGM_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                     generated_exp = PGM_exp_original)

    ##after perturbation
    PGM_exp_perturbed = PGM_explainer.get_explanation_node(node_idx = node_idx, 
                                                           x = data.x, 
                                                           edge_index = perturbed_edge_index,)
    PGM_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                      generated_exp = PGM_exp_perturbed)
    
    print('Explanation Accuracy Change for PGMExplainer:\n {:.4f} -> {:.4f}'.format(PGM_acc_original, PGM_acc_perturbed))
    
    
    # --------------------------------
    # # Explainer9: RandomExplainer
    # --------------------------------
    from graphxai.explainers import RandomExplainer
        
    # No training with Integrated Gradients, just run the model:
    random_explainer = RandomExplainer(prediction_model)
    
    ##before perturbation
    random_exp_original = random_explainer.get_explanation_node(node_idx = node_idx, 
                                                                x = data.x, 
                                                                edge_index = data.edge_index)
    
    random_acc_original = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                        generated_exp = random_exp_original)

    ##after perturbation
    random_exp_perturbed = random_explainer.get_explanation_node(node_idx = node_idx, 
                                                                 x = data.x, 
                                                                 edge_index = perturbed_edge_index,)
    random_acc_perturbed = graph_exp_acc(gt_exp = ground_truth_exp_subgraph[0], 
                                         generated_exp = random_exp_perturbed)
    
    print('Explanation Accuracy Change for RandomExplainer:\n {:.4f} -> {:.4f}'.format(random_acc_original, random_acc_perturbed))
        
    # --------------------------------
    # # Append results for all explainers
    # --------------------------------
    # At the end of each iteration, collect the results for the current node
    attack_results_prob5.append({
                'node_idx': node_idx,
                'used_budget': used_budget,
                'pg_acc_original': pg_acc_original,
                'pg_acc_perturbed': pg_acc_perturbed,
                'grad_acc_original': grad_acc_original,
                'grad_acc_perturbed': grad_acc_perturbed,
                'gCAM_acc_original': gCAM_acc_original,
                'gCAM_acc_perturbed': gCAM_acc_perturbed,
                'gbp_acc_original': gbp_acc_original,
                'gbp_acc_perturbed': gbp_acc_perturbed,
                'ig_acc_original': ig_acc_original,
                'ig_acc_perturbed': ig_acc_perturbed,
                'gnnE_acc_original': gnnE_acc_original,
                'gnnE_acc_perturbed': gnnE_acc_perturbed,
                'subX_acc_original': subX_acc_original,
                'subX_acc_perturbed': subX_acc_perturbed,
                'PGM_acc_original': PGM_acc_original,
                'PGM_acc_perturbed': PGM_acc_perturbed,
                'random_acc_original': random_acc_original,
                'random_acc_perturbed': random_acc_perturbed
                })
    
import pandas as pd
# Convert the list of results into a DataFrame
attack_results_prob5 = pd.DataFrame(attack_results_prob5)


# =============================================================================
# Some results analysis for Table 3
# =============================================================================
import ast
import pandas as pd

# df_all = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)

df1 = attack_results_prob5

mydata = df1


# Function to parse the string representation of lists and convert to floats
def parse_and_convert_to_floats(list_str):
    return [float(num) for num in ast.literal_eval(list_str)]

# Convert the columns from string representations of lists to actual lists of floats, if they are not already lists
def convert_if_needed(item):
    if isinstance(item, str):
        return parse_and_convert_to_floats(item)
    return item


# Calculate various metrics

##--------------------------
##For PGExplainer
##--------------------------
pg_acc_original = mydata["pg_acc_original"].mean()  ## Original Explanation Accuracy

pg_acc_perturbed = mydata["pg_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
pg_acc_change = pg_acc_original - pg_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For GradExplainer
##--------------------------
grad_acc_original = mydata["grad_acc_original"].mean()  ## Original Explanation Accuracy

grad_acc_perturbed = mydata["grad_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
grad_acc_change = grad_acc_original - grad_acc_perturbed  ## Explanation Accuracy Change

##--------------------------
##For GradCAM
##--------------------------
gCAM_acc_original = mydata["gCAM_acc_original"].mean()  ## Original Explanation Accuracy

gCAM_acc_perturbed = mydata["gCAM_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
gCAM_acc_change = gCAM_acc_original - gCAM_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For GuidedBP
##--------------------------
gbp_acc_original = mydata["gbp_acc_original"].mean()  ## Original Explanation Accuracy

gbp_acc_perturbed = mydata["gbp_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
gbp_acc_change = gbp_acc_original - gbp_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For IntegratedGradExplainer
##--------------------------

ig_acc_original = mydata["ig_acc_original"].mean()  ## Original Explanation Accuracy

ig_acc_perturbed = mydata["ig_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
ig_acc_change = ig_acc_original - ig_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For GNNExplainer
##--------------------------

gnnE_acc_original = mydata["gnnE_acc_original"].mean()  ## Original Explanation Accuracy

gnnE_acc_perturbed = mydata["gnnE_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
gnnE_acc_change = gnnE_acc_original - gnnE_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For SubgraphX
##--------------------------

subX_acc_original = mydata["subX_acc_original"].mean()  ## Original Explanation Accuracy

subX_acc_perturbed = mydata["subX_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
subX_acc_change = subX_acc_original - subX_acc_perturbed  ## Explanation Accuracy Change


##--------------------------
##For PGMExplainer
##--------------------------

PGM_acc_original = mydata["PGM_acc_original"].mean()  ## Original Explanation Accuracy

PGM_acc_perturbed = mydata["PGM_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
PGM_acc_change = PGM_acc_original - PGM_acc_perturbed  ## Explanation Accuracy Change

##--------------------------
##For RandomExplainer
##--------------------------

random_acc_original = mydata["random_acc_original"].mean()  ## Original Explanation Accuracy

random_acc_perturbed = mydata["random_acc_perturbed"].mean()  ## Perturbed Explanation Accuracy
random_acc_change = random_acc_original - random_acc_perturbed  ## Explanation Accuracy Change



# Print all results formatted as percentages only in the print function
print("Transfer attack results are as follows: \n")

print(f"Original Explanation Accuracy for PGExplainer: {pg_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for PGExplainer: {pg_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for PGExplainer: {pg_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for GradExplainer: {grad_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for GradExplainer: {grad_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for GradExplainer: {grad_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for GradCAM: {gCAM_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for GradCAM: {gCAM_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for GradCAM: {gCAM_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for GuidedBP: {gbp_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for GuidedBP: {gbp_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for GuidedBP: {gbp_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for IGExplainer: {ig_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for IGExplainer: {ig_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for IGExplainer: {ig_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for GNNExplainer: {gnnE_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for GNNExplainer: {gnnE_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for GNNExplainer: {gnnE_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for SubgraphX: {subX_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for SubgraphX: {subX_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for SubgraphX: {subX_acc_change * 100:.1f}%")

print(f"Original Explanation Accuracy for PGMExplainer: {PGM_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for PGMExplainer: {PGM_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for PGMExplainer: {PGM_acc_change * 100:.1f}%")


print(f"Original Explanation Accuracy for RandomExplainer: {random_acc_original * 100:.1f}%")
print(f"Perturbed Explanation Accuracy for RandomExplainer: {random_acc_perturbed * 100:.1f}%")
print(f"Explanation Accuracy Change for RandomExplainer: {random_acc_change * 100:.1f}%")

# =============================================================================
# Save results into dataframe
# =============================================================================

# Collect data for each metric in a dictionary with the variable names as keys
results_data = {
    "pg_acc_original": f"{pg_acc_original * 100:.1f}%",
    "pg_acc_perturbed": f"{pg_acc_perturbed * 100:.1f}%",
    "pg_acc_change": f"{pg_acc_change * 100:.1f}%",
    "grad_acc_original": f"{grad_acc_original * 100:.1f}%",
    "grad_acc_perturbed": f"{grad_acc_perturbed * 100:.1f}%",
    "grad_acc_change": f"{grad_acc_change * 100:.1f}%",
    "gCAM_acc_original": f"{gCAM_acc_original * 100:.1f}%",
    "gCAM_acc_perturbed": f"{gCAM_acc_perturbed * 100:.1f}%",
    "gCAM_acc_change": f"{gCAM_acc_change * 100:.1f}%",
    "gbp_acc_original": f"{gbp_acc_original * 100:.1f}%",
    "gbp_acc_perturbed": f"{gbp_acc_perturbed * 100:.1f}%",
    "gbp_acc_change": f"{gbp_acc_change * 100:.1f}%",
    "ig_acc_original": f"{ig_acc_original * 100:.1f}%",
    "ig_acc_perturbed": f"{ig_acc_perturbed * 100:.1f}%",
    "ig_acc_change": f"{ig_acc_change * 100:.1f}%",
    "gnnE_acc_original": f"{gnnE_acc_original * 100:.1f}%",
    "gnnE_acc_perturbed": f"{gnnE_acc_perturbed * 100:.1f}%",
    "gnnE_acc_change": f"{gnnE_acc_change * 100:.1f}%",
    "subX_acc_original": f"{subX_acc_original * 100:.1f}%",
    "subX_acc_perturbed": f"{subX_acc_perturbed * 100:.1f}%",
    "subX_acc_change": f"{subX_acc_change * 100:.1f}%",
    "PGM_acc_original": f"{PGM_acc_original * 100:.1f}%",
    "PGM_acc_perturbed": f"{PGM_acc_perturbed * 100:.1f}%",
    "PGM_acc_change": f"{PGM_acc_change * 100:.1f}%",
    "random_acc_original": f"{random_acc_original * 100:.1f}%",
    "random_acc_perturbed": f"{random_acc_perturbed * 100:.1f}%",
    "random_acc_change": f"{random_acc_change * 100:.1f}%"
}




# Convert the dictionary into a DataFrame
results_df = pd.DataFrame([results_data])

# Optionally, you can transpose the DataFrame to have metrics as rows and a single column for values
results_df = results_df.T
results_df.columns = ['Value']  # Rename the column


# # Optionally, save the DataFrame to a CSV file
results_df.to_csv(f"transfer_analysis_results_{args.dataset_name}.csv", header=True, index_label="Metric")


